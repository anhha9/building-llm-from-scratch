# Building an LLM from Scratch

## Overview
In the past two years, I have been keeping up with and implementing some Artificial Intelligence models, especially Large Language Models (LLM). Concerning LLMs, I had experience adapting them through prompt engineering and fine-tuning. However, I want to implement an LLM from scratch to explore every component in an LLM so that I can get a deeper understanding of LLMs. For example, I am very interested in the tokenizer of LLMs, because this component plays a significant part in helping the LLMs understand input text.

To achieve impressive results like ChatGPT's GPT-4o, significant amounts of resources, ranging from data to computing budget, are needed. Therefore, as an individual project, the resulting model from my implementation will not generate impressive, state-of-the-art results, as I only train my model on laptop and free Colab and Kaggle budgets. 

In this project, I will implement GPT-2 model, based on the paper by [OpenAI](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) and [Sebastian Raschka's Tutorials](https://github.com/rasbt/LLMs-from-scratch)


## Contents
- Code folder: containing all the code to build GPT-2
- Data folder: containing the pre-training data